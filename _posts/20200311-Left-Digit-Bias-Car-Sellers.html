---
layout: post
title: Left Digit Bias in a Car Purchase
excerpt: Why is it $2.99 and not $3.00?
permalink: /Left-Digit-Bias-Car-Sellers
publish: false
tags: [Left Digit Bias, Cars, linear regression, logistic regression]
images:
  - url: /images/creditcard_fraud/cardthief.gif
---
<link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">

<h2><span style="text-decoration: underline;"><strong>Introduction</strong></span></h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In your last trip to the grocery store, you probably noticed many prices end in “99.” like this one here 
<a href = "https://www.globalprice.info/en/?p=usa/food-prices-in-usa" target="_blank">Food Prices.</a>
 Why do marketers price products this way instead of rounding prices to the nearest whole dollar? 
 While it may seem surprising, such pricing strategies have been very effective in increasing sales and consequently profits as well, by taking the advantage of the left-digit bias. 
 The left-digit bias describes a phenomenon in which consumer's perceptions and evaluations are disproportionately influenced by the left-most digit of the product price.
 But when is this bias more likely to exert influence on consumers?</p>
<p>In this article, we will be looking at implementing linear and logistic regression models for detecting the presence of left-digit bias.</p>

<h2><span style="text-decoration: underline;"><strong>Dataset</strong></span></h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The dataset used in this post is from course work of Data Driven Marketing.<a href = "https://github.com/saiteja-2811/DDM---Left-Digit-Bias---Cars" target="_blank"></a>
 It consists of the attributes of second-hand cars and their sold/unsold status. Due to confidentiality issues, the original features were transformed and shifted.
 The features given in the dataset consists of 7 independent variables.
 'Mile' is the number of miles the car had travelled and the 'Price' refers to the selling price of the car.
 The response variable, ‘Sold’ is a binary variable indicating 1 for a sold car and 0 otherwise.
</p>

<h2><span style="text-decoration: underline;"><strong>Approach</strong></span></h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp The objective is to detect the presence of left-digit bias or not when the customer plans to purchase a car. 
In Machine Learning terms, this is a classification problem. Here, we will implement a logistic regression model</p>

<h4>Logistic Regression </h4>
<p>Logistic regression is a type of classification algorithm which is being used since early twentieth century. The algorithm tries to find relationship between the features and probability of a particular outcome.
Logistic regression is further divided into various sub-types – binary, multinomial, ordinal; depending upon the type of response variable.
In this post, we will build a binary logistic model as our response variable is binary - ‘Sold’ or ‘Not Sold’.
</p>

<h2><span style="text-decoration: underline;"><strong>Data Preparation</strong></span></h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp We will prepare our data for training. Load the required libraries and read the csv file. You can download the dataset from <a href = "https://github.com/saiteja-2811/DDM---Left-Digit-Bias---Cars" target = "_blank">here.</a>
</p>
<img src="images/creditcard_fraud/code1.PNG">
<p> As discussed before there are 7 independent variables, but we have modelyear and month as factor variables. We set a base for these factor variables and proceed to make the model. Class is our response variable. There are 945 rows and 8 columns.</p>
<img src="images/creditcard_fraud/code2.PNG">
<p>There are no missing values in the dataset. Let’s look at the proportion of classes.</p>
<img src="images/creditcard_fraud/code3.PNG">
<p>Woah! 284,315 out of 284,807 observations (99.8%) of the data is of class ‘Not Fraud’. Even if we predict all the transactions as not fraudulent, we will have a 99.8% accuracy and this is why accuracy is not a good measure in classification problems.</p>
<p>Let's look at the distribution of 'Amount' and 'Time' features.</p>
<img src="images/creditcard_fraud/code4.PNG">
<p>The distribution of transaction amount shows that it is skewed. We will apply a log transformation to correct it.</p>
<img src="images/creditcard_fraud/code5.PNG">
<img src="images/creditcard_fraud/code6.PNG">
<p>Transaction time distribution shows that most transactions happened during day time. Since transaction time is on different scale as compared to other features in our dataset, we need to scale the data. Also, the function for logistic regression used in this post uses regularization which is largely affected by the scale of data.</p>
<img src="images/creditcard_fraud/code7.PNG">
<p>We will now split our dataset for training and testing in the ratio 70:30. Our data is highly imbalanced. If we use the same data for training the model, it will be biased towards the ‘0’ class. To account for this, we need to oversample the class ‘1’ observations so that a balanced dataset can be used for training.</p>
<p> We will use SMOTE to create a balanced dataset. SMOTE stands for Synthetic Minority Over-sampling Technique. To put simply, SMOTE does not create duplicate observations, it creates synthetic samples by selecting similar records and altering the columns one by one by a random amount within the difference between the adjacent records. </p>
<img src="images/creditcard_fraud/code8.PNG">
<p>Ahaa! There’s our balanced set with equal propotions of both classes ( 199,032 observations each).</p>
<p>Do we require all the features in our dataset? We don’t know! Let’s run a recursive feature elimination (RFE) and use only the best features in our model. RFE repeatedly constructs a model by choosing the best or worst performing feature, setting aside the chosen one and continuing the process with the rest of the features. Features are ranked according to when they are eliminated.</p>
<img src="images/creditcard_fraud/code9.PNG">
<p>Let's check the p-values of the selected features using the statsmodel library. The output indicates that all the p-values are zero indicating that the features selected are significant.</p>
<img src="images/creditcard_fraud/code10.PNG">

<h2><span style="text-decoration: underline;"><strong>Training the model</strong></span></h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp Let’s build the model with 15 features selected by RFE and test it.</p>
<img src="images/creditcard_fraud/code11.PNG">
<p>What is the confusion matrix indicating?</p>
<p>We 83,070 observations which are 'Not fradulent' and are predicted as 'Not fraudulent', </br>
2213 observations which are 'Not fraudulent' but are predicted as 'Fraudulent',</br>
13 observations which are 'Fraudulent' and predicted as ‘Not fraudulent’,</br>
147 observations which are 'Fraudulent' and predicted as 'Fraudulent'. </br>Our aim is to improve the model so as to decrease the number of wrongly classified observations especially the fraudulent ones.</p>
<p>We can improve the model by changing the ‘C’ value (the inverse of regularization strength) and the regularization method. To find the best 'C' value we will do a grid search using the GridSearchCV function. It will search among the all possible combinations of parameters given and calculates scores using validation data and outputs the best parameters. </p>
<img src="images/creditcard_fraud/code12.PNG">
<p>Using the grid search, we get C = 0.01 and “l2” as the best regularization method in this case. Let’s re-train the model using these parameters. </p>
<img src="images/creditcard_fraud/code13.PNG">
<h2><span style="text-decoration: underline;"><strong>Testing</strong></span></h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp On using our new model on test data, we observe that the number of wrongly classified genuine transactions have decreased but the fradulent ones remain the same.</p>
<img src="images/creditcard_fraud/code14.PNG">
<p>Some of the other evaluation metrics are recall, f1-score and ROC curve.</p>
<img src="images/creditcard_fraud/code15.PNG">
<img src="images/creditcard_fraud/code16.PNG">
<p>The recall and F-1 score are satisfactory. The ROC is also closer towards the top border indicating the model is effective.</br>
Can think of any other ways to predict fraudulent transactions? Build the model and let me know the recall value in the comments!</p>
